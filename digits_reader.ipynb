{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset using keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist = tf.keras.datasets.mnist\n",
    "# (x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset manually \n",
    "- Download the data manually from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
    "- Create a folder in a location that you have saved your jupyter notebook and call it dataset\n",
    "- Move the downloaded file from 'Downloads' to 'dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x_test', 'x_train', 'y_train', 'y_test']\n"
     ]
    }
   ],
   "source": [
    "# Load dataset manually \n",
    "mnist = np.load('dataset/mnist.npz')\n",
    "print(mnist.files)\n",
    "\n",
    "x_train = mnist['x_train']\n",
    "y_train = mnist['y_train']\n",
    "x_test = mnist['x_test'] \n",
    "y_test = mnist['y_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Exploration\n",
    "In this workshop, we will use the 'mnist dataset' to classify the hand written digits images. In this dataset, there are 70000 images. As digits are from 0 to 9, therefore there are 10 unique digits in this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:  (60000, 28, 28)\n",
      "y_train shape:  (60000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"x_train shape: \" , x_train.shape)\n",
    "print(\"y_train shape: \" , y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The shape of the x_train is (60000, 28, 28) <br>\n",
    "> 60000 means that we have 60000 images<br>\n",
    "> 28 means that our image size is 28x28 (28x28 pixels)<br>\n",
    "- The shape of the Y is (60000,)<br>\n",
    "> 60000 means that we have 60000 labels (10 unique lables from 0 to 9) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's visualize what is in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 27.5, 27.5, -0.5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_size = 28\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.imshow(x_train[100].reshape(img_size, img_size), cmap = plt.cm.binary)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.imshow(x_train[900].reshape(img_size, img_size), cmap = plt.cm.binary)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.imshow(x_train[670].reshape(img_size, img_size), cmap = plt.cm.binary)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.imshow(x_train[1999].reshape(img_size, img_size), cmap = plt.cm.binary)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize the data (scale it between 0 and 1)\n",
    "Let's first take a look at one sample in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
       "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
       "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
       "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
       "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
       "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
       "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
       "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
       "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
       "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
       "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
       "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
       "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
       "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the printed array, currently our pixels (features) are between 0 and 255. When we normalize our data, pixels will be between 0 and 1. Therefore, our network will learn easier! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tf.keras.utils.normalize(x_train, axis=1)\n",
    "x_test = tf.keras.utils.normalize(x_test, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see how our data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.00393124, 0.02332955, 0.02620568,\n",
       "        0.02625207, 0.17420356, 0.17566281, 0.28629534, 0.05664824,\n",
       "        0.51877786, 0.71632322, 0.77892406, 0.89301644, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.05780486, 0.06524513,\n",
       "        0.16128198, 0.22713296, 0.22277047, 0.32790981, 0.36833534,\n",
       "        0.3689874 , 0.34978968, 0.32678448, 0.368094  , 0.3747499 ,\n",
       "        0.79066747, 0.67980478, 0.61494005, 0.45002403, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.12250613, 0.45858525, 0.45852825,\n",
       "        0.43408872, 0.37314701, 0.33153488, 0.32790981, 0.36833534,\n",
       "        0.3689874 , 0.34978968, 0.32420121, 0.15214552, 0.17865984,\n",
       "        0.25626376, 0.1573102 , 0.12298801, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.04500225, 0.4219755 , 0.45852825,\n",
       "        0.43408872, 0.37314701, 0.33153488, 0.32790981, 0.28826244,\n",
       "        0.26543758, 0.34149427, 0.31128482, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.1541463 , 0.28272888,\n",
       "        0.18358693, 0.37314701, 0.33153488, 0.26569767, 0.01601458,\n",
       "        0.        , 0.05945042, 0.19891229, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.0253731 ,\n",
       "        0.00171577, 0.22713296, 0.33153488, 0.11664776, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.20500962, 0.33153488, 0.24625638, 0.00291174,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.01622378, 0.24897876, 0.32790981, 0.10191096,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.04586451, 0.31235677, 0.32757096,\n",
       "        0.23335172, 0.14931733, 0.00129164, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.10498298, 0.34940902,\n",
       "        0.3689874 , 0.34978968, 0.15370495, 0.04089933, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.06551419,\n",
       "        0.27127137, 0.34978968, 0.32678448, 0.245396  , 0.05882702,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.02333517, 0.12857881, 0.32549285, 0.41390126, 0.40743158,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.32161793, 0.41390126, 0.54251585,\n",
       "        0.20001074, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.06697006,\n",
       "        0.18959827, 0.25300993, 0.32678448, 0.41390126, 0.45100715,\n",
       "        0.00625034, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.05110617, 0.19182076, 0.33339444,\n",
       "        0.3689874 , 0.34978968, 0.32678448, 0.40899334, 0.39653769,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.04117838, 0.16813739, 0.28960162, 0.32790981, 0.36833534,\n",
       "        0.3689874 , 0.34978968, 0.25961929, 0.12760592, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.04431706, 0.11961607,\n",
       "        0.36545809, 0.37314701, 0.33153488, 0.32790981, 0.36833534,\n",
       "        0.28877275, 0.111988  , 0.00258328, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.05298497, 0.42752138, 0.4219755 , 0.45852825,\n",
       "        0.43408872, 0.37314701, 0.33153488, 0.25273681, 0.11646967,\n",
       "        0.01312603, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.37491383,\n",
       "        0.56222061, 0.66525569, 0.63253163, 0.48748768, 0.45852825,\n",
       "        0.43408872, 0.359873  , 0.17428513, 0.01425695, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.92705966,\n",
       "        0.82698729, 0.74473314, 0.63253163, 0.4084877 , 0.24466922,\n",
       "        0.22648107, 0.02359823, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's take a look at our data again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 28\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.imshow(x_train[100].reshape(img_size, img_size), cmap = plt.cm.binary)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.imshow(x_train[900].reshape(img_size, img_size), cmap = plt.cm.binary)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.imshow(x_train[670].reshape(img_size, img_size), cmap = plt.cm.binary)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.imshow(x_train[1999].reshape(img_size, img_size), cmap = plt.cm.binary)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Exploration\n",
    "In this workshop, we will use the 'mnist dataset' to classify the hand written digits images. In this dataset, there are 70000 images. As digits are from 0 to 9, therefore there are 10 unique digits in this data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"x_train shape: \" , x_train.shape)\n",
    "print(\"y_train shape: \" , y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The shape of the x_train is (60000, 28, 28) <br>\n",
    "> 60000 means that we have 60000 images<br>\n",
    "> 28 means that our image size is 28x28 (28x28 pixels)<br>\n",
    "- The shape of the Y is (60000,)<br>\n",
    "> 60000 means that we have 60000 labels (10 unique lables from 0 to 9) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let's build our feed forward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a basic feed-forward model\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# Input layer: Our input array (X) is a 3 dimensional array. \n",
    "#In order to be able to use it in our first deep learning model, \n",
    "#we need to make it flatten (2D).So, we need to take this 28x28 image, and make it a flat 1x784\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "# a simple fully-connected layer, 128 units, relu activation\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "\n",
    "# a simple fully-connected layer, 128 units, relu activation\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "\n",
    "# our output layer. 10 units for 10 classes. Softmax for probability distribution\n",
    "model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model \n",
    "model.fit(x_train, y_train, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if the model generalize or overfit\n",
    "val_loss, val_acc = model.evaluate(x_test, y_test)\n",
    "print(val_loss)\n",
    "print(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('digit.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_model = tf.keras.models.load_model('digit.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = digit_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.argmax(predictions[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 28\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.imshow(x_test[10].reshape(img_size, img_size), cmap = plt.cm.binary)\n",
    "plt.axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
